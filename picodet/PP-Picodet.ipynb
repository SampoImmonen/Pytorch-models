{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "assisted-oxygen",
   "metadata": {},
   "source": [
    "# Notebook to build PP-PicoDet model with basic pytorch building blocks\n",
    "- https://arxiv.org/abs/2111.00902\n",
    "\n",
    "## Note new version picodetv2\n",
    "- https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.4/ppdet/modeling/architectures/picodet.py\n",
    "- LCNET backbone (https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.4/ppdet/modeling/backbones/lcnet.py)\n",
    "- LCPAN (https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.4/ppdet/modeling/necks/lc_pan.py)\n",
    "- picoheadv2 (https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.4/ppdet/modeling/heads/pico_head.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "disabled-respect",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-soldier",
   "metadata": {},
   "source": [
    "# Backbone PPLCNET (https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.4/ppdet/modeling/backbones/lcnet.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "blond-protocol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 160, 160])\n"
     ]
    }
   ],
   "source": [
    "# utils \n",
    "NET_CONFIG = {\n",
    "    \"blocks2\":\n",
    "    #k, in_c, out_c, s, use_se\n",
    "    [[3, 16, 32, 1, False], ],\n",
    "    \"blocks3\": [\n",
    "        [3, 32, 64, 2, False],\n",
    "        [3, 64, 64, 1, False],\n",
    "    ],\n",
    "    \"blocks4\": [\n",
    "        [3, 64, 128, 2, False],\n",
    "        [3, 128, 128, 1, False],\n",
    "    ],\n",
    "    \"blocks5\": [\n",
    "        [3, 128, 256, 2, False],\n",
    "        [5, 256, 256, 1, False],\n",
    "        [5, 256, 256, 1, False],\n",
    "        [5, 256, 256, 1, False],\n",
    "        [5, 256, 256, 1, False],\n",
    "        [5, 256, 256, 1, False],\n",
    "    ],\n",
    "    \"blocks6\": [[5, 256, 512, 2, True], [5, 512, 512, 1, True]]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def make_divisible(v, divisor=8, min_value=None):\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "class ConvBNLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_channels,\n",
    "                 filter_size,\n",
    "                 num_filters,\n",
    "                 stride,\n",
    "                 num_groups=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels=num_channels,\n",
    "                              out_channels=num_filters,\n",
    "                              kernel_size=filter_size,\n",
    "                              stride=stride,\n",
    "                              padding=(filter_size-1)//2,\n",
    "                              groups=num_groups,\n",
    "                              bias=False)\n",
    "        # in inference fuse to conv\n",
    "        self.bn = nn.BatchNorm2d(num_filters)\n",
    "        self.hardswish = nn.Hardswish()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.hardswish(x)\n",
    "        return x\n",
    "\n",
    "class SEModule(nn.Module):\n",
    "    def __init__(self, channel, reduction=4):\n",
    "        super().__init__()\n",
    "        self.avg_pool= nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv1 = nn.Conv2d(in_channels=channel,\n",
    "                               out_channels=channel//reduction,\n",
    "                               kernel_size=1,\n",
    "                               stride=1,\n",
    "                               padding=0)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(in_channels=channel//reduction,\n",
    "                               out_channels=channel,\n",
    "                               kernel_size=1,\n",
    "                               stride=1,\n",
    "                               padding=0)\n",
    "        self.hardsigmoid = nn.Hardsigmoid()\n",
    "    def forward(self, x):\n",
    "        idendity = x\n",
    "        x = self.avg_pool(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.hardsigmoid(x)\n",
    "        x = x*idendity\n",
    "        return x\n",
    "        \n",
    "        \n",
    "class DepthWiseSeparable(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_channels,\n",
    "                 num_filters,\n",
    "                 stride,\n",
    "                 dw_size=3,\n",
    "                 use_se=False):\n",
    "        super().__init__()\n",
    "        self.use_se = use_se\n",
    "        self.dw_conv = ConvBNLayer(num_channels=num_channels,\n",
    "                                   num_filters=num_channels,\n",
    "                                   filter_size=dw_size,\n",
    "                                   stride=stride,\n",
    "                                   num_groups=num_channels)\n",
    "        if use_se:\n",
    "            self.se = SEModule(num_channels)\n",
    "        self.pw_conv = ConvBNLayer(num_channels=num_channels,\n",
    "                                   filter_size=1,\n",
    "                                   num_filters=num_filters,\n",
    "                                   stride=1)\n",
    "    def forward(self, x):\n",
    "        x = self.dw_conv(x)\n",
    "        if self.use_se:\n",
    "            x = self.se(x)\n",
    "        x = self.pw_conv(x)\n",
    "        return x\n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "test_scale = 1\n",
    "conv_bn = ConvBNLayer(num_channels=3,\n",
    "                      filter_size=3,\n",
    "                      num_filters=make_divisible(16*test_scale),\n",
    "                      stride=2)\n",
    "se_module = SEModule(make_divisible(16*test_scale))\n",
    "blocks2 = nn.Sequential(*[DepthWiseSeparable(\n",
    "                            num_channels=make_divisible(in_c*test_scale),\n",
    "                            num_filters=make_divisible(out_c*test_scale),\n",
    "                            dw_size=k,\n",
    "                            stride=s,\n",
    "                            use_se=se) for i, (k, in_c, out_c, s, se) in enumerate(NET_CONFIG[\"blocks2\"])])\n",
    "\n",
    "test_input = torch.randn(1, 3, 320, 320)\n",
    "test_out = conv_bn(test_input)\n",
    "test_out = blocks2(test_out)\n",
    "print(test_out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "identified-correction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 40, 40])\n",
      "torch.Size([1, 256, 20, 20])\n",
      "torch.Size([1, 512, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "class LCNet(nn.Module):\n",
    "    def __init__(self, scale=1.0, feature_maps=[3, 4, 5]):\n",
    "        super().__init__()\n",
    "        self.scale = scale\n",
    "        self.feature_maps = feature_maps\n",
    "        \n",
    "        out_channels = []\n",
    "        self.conv1 = ConvBNLayer(num_channels=3,\n",
    "                                 filter_size=3,\n",
    "                                 num_filters=make_divisible(16*scale),\n",
    "                                 stride=2)\n",
    "        self.blocks2 = nn.Sequential(*[DepthWiseSeparable(\n",
    "                            num_channels=make_divisible(in_c*scale),\n",
    "                            num_filters=make_divisible(out_c*scale),\n",
    "                            dw_size=k,\n",
    "                            stride=s,\n",
    "                            use_se=se) for i, (k, in_c, out_c, s, se) in enumerate(NET_CONFIG[\"blocks2\"])])\n",
    "        self.blocks3 = nn.Sequential(*[DepthWiseSeparable(\n",
    "                            num_channels=make_divisible(in_c*scale),\n",
    "                            num_filters=make_divisible(out_c*scale),\n",
    "                            dw_size=k,\n",
    "                            stride=s,\n",
    "                            use_se=se) for i, (k, in_c, out_c, s, se) in enumerate(NET_CONFIG[\"blocks3\"])])\n",
    "        \n",
    "        out_channels.append(make_divisible(NET_CONFIG[\"blocks3\"][-1][2]*scale))\n",
    "        \n",
    "        self.blocks4 = nn.Sequential(*[DepthWiseSeparable(\n",
    "                            num_channels=make_divisible(in_c*scale),\n",
    "                            num_filters=make_divisible(out_c*scale),\n",
    "                            dw_size=k,\n",
    "                            stride=s,\n",
    "                            use_se=se) for i, (k, in_c, out_c, s, se) in enumerate(NET_CONFIG[\"blocks4\"])])\n",
    "        \n",
    "        out_channels.append(make_divisible(NET_CONFIG[\"blocks4\"][-1][2]*scale))\n",
    "        \n",
    "        self.blocks5 = nn.Sequential(*[DepthWiseSeparable(\n",
    "                            num_channels=make_divisible(in_c*scale),\n",
    "                            num_filters=make_divisible(out_c*scale),\n",
    "                            dw_size=k,\n",
    "                            stride=s,\n",
    "                            use_se=se) for i, (k, in_c, out_c, s, se) in enumerate(NET_CONFIG[\"blocks5\"])])\n",
    "        \n",
    "        out_channels.append(make_divisible(NET_CONFIG[\"blocks5\"][-1][2]*scale))\n",
    "        \n",
    "        self.blocks6 = nn.Sequential(*[DepthWiseSeparable(\n",
    "                            num_channels=make_divisible(in_c*scale),\n",
    "                            num_filters=make_divisible(out_c*scale),\n",
    "                            dw_size=k,\n",
    "                            stride=s,\n",
    "                            use_se=se) for i, (k, in_c, out_c, s, se) in enumerate(NET_CONFIG[\"blocks6\"])])\n",
    "        \n",
    "        out_channels.append(make_divisible(NET_CONFIG[\"blocks6\"][-1][2]*scale))\n",
    "        self._out_channels = [\n",
    "            ch for idx, ch in enumerate(out_channels) if idx+2 in feature_maps]\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = inputs['image']\n",
    "        outs = []\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.blocks2(x)\n",
    "        x = self.blocks3(x)\n",
    "        outs.append(x)\n",
    "        x = self.blocks4(x)\n",
    "        outs.append(x)\n",
    "        x = self.blocks5(x)\n",
    "        outs.append(x)\n",
    "        x = self.blocks6(x)\n",
    "        outs.append(x)\n",
    "        outs = [o for i, o in enumerate(outs) if i+2 in self.feature_maps]\n",
    "        return outs\n",
    "\n",
    "backbone = LCNet()\n",
    "inp_t = torch.randn(1, 3, 320, 320)\n",
    "output = backbone({'image': inp_t})\n",
    "for t in output:\n",
    "    print(t.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promotional-palace",
   "metadata": {},
   "source": [
    "# Detector Neck LCPan: (https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.4/ppdet/modeling/necks/lc_pan.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "toxic-circular",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 96, 255, 255])\n",
      "torch.Size([1, 96, 255, 255])\n",
      "torch.Size([1, 96, 255, 255])\n",
      "torch.Size([1, 96, 25, 25])\n"
     ]
    }
   ],
   "source": [
    "class ConvBNLayerPAN(nn.Module):\n",
    "    \"\"\"\n",
    "    In Paddle Paddle there is two modules\n",
    "    named ConvBNLayer so we name this ConvBNLayerPAN\n",
    "    to separate the two\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "             in_channel=96,\n",
    "             out_channel=96,\n",
    "             kernel_size=3,\n",
    "             stride=1,\n",
    "             groups=1,\n",
    "             act='leaky_relu'):\n",
    "        super(ConvBNLayerPAN, self).__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=in_channel,\n",
    "            out_channels=out_channel,\n",
    "            kernel_size=kernel_size,\n",
    "            groups=groups,\n",
    "            padding=(kernel_size - 1) // 2,\n",
    "            stride=stride,\n",
    "            bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channel)\n",
    "        \n",
    "        self.has_act = False\n",
    "        if act:\n",
    "            self.has_act = True\n",
    "            \n",
    "        self.act = nn.LeakyReLU()\n",
    "        if act == \"hard_swish\":\n",
    "            self.act = nn.Hardswish()\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.bn(self.conv(x))\n",
    "        if self.act:\n",
    "            x = self.act(x)\n",
    "        return x\n",
    "    \n",
    "class Channel_T(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels=[116, 232, 464],\n",
    "                 out_channels=96,\n",
    "                 act=\"leaky_relu\"):\n",
    "        super(Channel_T, self).__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        for channel_count in in_channels:\n",
    "            self.convs.append(ConvBNLayerPAN(channel_count, out_channels, 1, act=act))\n",
    "    def forward(self, x):\n",
    "        outs = [self.convs[i](x[i]) for i in range(len(x))]\n",
    "        return outs\n",
    "            \n",
    "class DPModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Depth-wise and point-wise module.\n",
    "     Args:\n",
    "        in_channel (int): The input channels of this Module.\n",
    "        out_channel (int): The output channels of this Module.\n",
    "        kernel_size (int): The conv2d kernel size of this Module.\n",
    "        stride (int): The conv2d's stride of this Module.\n",
    "        act (str): The activation function of this Module,\n",
    "                   Now support `leaky_relu` and `hard_swish`.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "             in_channel=96,\n",
    "             out_channel=96,\n",
    "             kernel_size=3,\n",
    "             stride=1,\n",
    "             act='leaky_relu',\n",
    "             use_act_in_out=True):\n",
    "        super(DPModule, self).__init__()\n",
    "        self.use_act = False\n",
    "        if act:\n",
    "            self.use_act = True\n",
    "        self.use_act_in_out = use_act_in_out\n",
    "        self.dwconv = nn.Conv2d(\n",
    "            in_channels=in_channel,\n",
    "            out_channels=out_channel,\n",
    "            kernel_size=kernel_size,\n",
    "            groups=out_channel,\n",
    "            padding=(kernel_size - 1) // 2,\n",
    "            stride=stride,\n",
    "            bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channel)\n",
    "        self.pwconv = nn.Conv2d(\n",
    "            in_channels=out_channel,\n",
    "            out_channels=out_channel,\n",
    "            kernel_size=1,\n",
    "            groups=1,\n",
    "            padding=0,\n",
    "            bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
    "        self.act_func = nn.LeakyReLU()\n",
    "        if act == \"hard_swish\":\n",
    "            self.act_func = nn.Hardswish()\n",
    "    def forward(self, x):\n",
    "        x = self.bn1(self.dwconv(x))\n",
    "        if self.use_act:\n",
    "            x = self.act_func(x)\n",
    "        x = self.bn2(self.pwconv(x))\n",
    "        if self.use_act_in_out:\n",
    "            x = self.act_func(x)\n",
    "        return x\n",
    "            \n",
    "# test Channel_T\n",
    "channel_t = Channel_T(act='hard_swish')\n",
    "inp = [torch.randn(1, c, 255, 255) for c in [116, 232, 464]]\n",
    "out = channel_t(inp)\n",
    "for t in out:\n",
    "    print(t.size())\n",
    "\n",
    "# test DPModule\n",
    "dp_module = DPModule()\n",
    "inp = torch.randn(1, 96, 25, 25)\n",
    "out = dp_module(inp)\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "entitled-outreach",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LCPAN(nn.Module):\n",
    "    \"\"\"Path Aggregation Network with LCNet module.\n",
    "    Args:\n",
    "        in_channels (List[int]): Number of input channels per scale.\n",
    "        out_channels (int): Number of output channels (used at each scale)\n",
    "        kernel_size (int): The conv2d kernel size of this Module.\n",
    "        num_features (int): Number of output features of CSPPAN module.\n",
    "        num_csp_blocks (int): Number of bottlenecks in CSPLayer. Default: 1\n",
    "        use_depthwise (bool): Whether to depthwise separable convolution in\n",
    "            blocks. Default: True\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size=5,\n",
    "                 num_features=3,\n",
    "                 use_depthwise=True,\n",
    "                 act='hard_swish',\n",
    "                 spatial_scales=[0.125, 0.0625, 0.03125]):\n",
    "        super(LCPAN, self).__init__()\n",
    "        self.conv_t = Channel_T(in_channels, out_channels, act=act)\n",
    "        in_channels = [out_channels]*len(spatial_scales)\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.spatial_scales = spatial_scales\n",
    "        self.num_features = num_features\n",
    "        conv_func = DPModule if use_depthwise else ConvBNLayer\n",
    "        \n",
    "        NET_CONFIG = {\n",
    "            #k, in_c, out_c, stride, use_se\n",
    "            \"block1\": [\n",
    "                [kernel_size, out_channels * 2, out_channels * 2, 1, False],\n",
    "                [kernel_size, out_channels * 2, out_channels, 1, False],\n",
    "            ],\n",
    "            \"block2\": [\n",
    "                [kernel_size, out_channels * 2, out_channels * 2, 1, False],\n",
    "                [kernel_size, out_channels * 2, out_channels, 1, False],\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        if self.num_features == 4:\n",
    "            self.first_top_conv = conv_func(\n",
    "                in_channels[0], in_channels[0], kernel_size, stride=2, act=act)\n",
    "            self.second_top_conv = conv_func(\n",
    "                in_channels[0], in_channels[0], kernel_size, stride=2, act=act)\n",
    "            self.spatial_scales.append(self.spatial_scales[-1] / 2)\n",
    "        \n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.top_down_blocks = nn.ModuleList()\n",
    "        for idx in range(len(in_channels)-1 , 0, -1):\n",
    "            self.top_down_blocks.append(nn.Sequential(*[\n",
    "                DepthWiseSeparable(num_channels=in_c,\n",
    "                                   num_filters=out_c,\n",
    "                                   dw_size=k,\n",
    "                                   stride=s,\n",
    "                                   use_se=se) for i, (k, in_c, out_c, s, se) in enumerate(NET_CONFIG['block1'])\n",
    "            ]))\n",
    "            \n",
    "        self.downsamples = nn.ModuleList()\n",
    "        self.bottom_up_blocks = nn.ModuleList()\n",
    "        \n",
    "        for idx in range(len(in_channels)-1):\n",
    "            self.downsamples.append(conv_func(in_channels[idx],\n",
    "                                              in_channels[idx],\n",
    "                                              kernel_size=kernel_size,\n",
    "                                              stride=2,\n",
    "                                              act=act))\n",
    "            self.bottom_up_blocks.append(nn.Sequential(*[\n",
    "                DepthWiseSeparable(num_channels=in_c,\n",
    "                                   num_filters=out_c,\n",
    "                                   dw_size=k,\n",
    "                                   stride=s,\n",
    "                                   use_se=se) for i, (k, in_c, out_c, s, se) in enumerate(NET_CONFIG[\"block2\"])\n",
    "            ]))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs (tuple[Tensor]): input features.\n",
    "        Returns:\n",
    "            tuple[Tensor]: CSPPAN features.\n",
    "        \"\"\"\n",
    "        \n",
    "        assert len(inputs) == len(self.in_channels)\n",
    "        inputs = self.conv_t(inputs)\n",
    "        \n",
    "        # top-down path\n",
    "        inner_outs = [inputs[-1]]\n",
    "        for idx in range(len(self.in_channels)-1, 0, -1):\n",
    "            feat_heigh = inner_outs[0] #(sic)\n",
    "            feat_low = inputs[idx-1]\n",
    "            \n",
    "            upsample_feat = self.upsample(feat_heigh)\n",
    "            \n",
    "            inner_out = self.top_down_blocks[len(self.in_channels)-1-idx](\n",
    "                torch.cat((upsample_feat, feat_low), dim=1))\n",
    "            inner_outs.insert(0, inner_out)\n",
    "        \n",
    "        # bottom-up path\n",
    "        outs = [inner_outs[0]]\n",
    "        for idx in range(len(self.in_channels)-1):\n",
    "            feat_low = outs[-1]\n",
    "            feat_height = inner_outs[idx+1]\n",
    "            downsample_feat = self.downsamples[idx](feat_low)\n",
    "            out = self.bottom_up_blocks[idx](\n",
    "                torch.cat((downsample_feat, feat_height), dim=1))\n",
    "\n",
    "            outs.append(out)\n",
    "        \n",
    "        top_features = None\n",
    "        if self.num_features == 4:\n",
    "            top_features = self.first_top_conv(inputs[-1])\n",
    "            top_features = top_features+self.second_top_conv(outs[-1])\n",
    "            outs.append(top_features)\n",
    "        return tuple(outs)\n",
    "lcpan = LCPAN(in_channels=[128, 256, 512], out_channels=96)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-mouth",
   "metadata": {},
   "source": [
    "# Test backbone plus LCPAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "funded-cache",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 96, 40, 40])\n",
      "torch.Size([1, 96, 20, 20])\n",
      "torch.Size([1, 96, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "# model config\n",
    "scale = 0.75\n",
    "feature_maps = [3, 4, 5]\n",
    "out_channels = 96\n",
    "\n",
    "backbone = LCNet(scale, feature_maps)\n",
    "# get backbone output shape\n",
    "outputs = backbone({'image': torch.randn(1, 3, 320, 320)})\n",
    "in_channels = [c.size()[1] for c in outputs]\n",
    "\n",
    "neck = LCPAN(in_channels, out_channels)\n",
    "outputs = neck(outputs)\n",
    "for out in outputs:\n",
    "    print(out.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "champion-purchase",
   "metadata": {},
   "source": [
    "# PicoHeadV2: https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.4/ppdet/modeling/heads/pico_head.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dental-turning",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvNormLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "             ch_in,\n",
    "             ch_out,\n",
    "             filter_size,\n",
    "             stride,\n",
    "             groups=1,\n",
    "             norm_type='bn',\n",
    "             norm_decay=0.,\n",
    "             norm_groups=32,\n",
    "             use_dcn=False,\n",
    "             bias_on=False,\n",
    "             lr_scale=1.,\n",
    "             freeze_norm=False,\n",
    "             skip_quant=False,\n",
    "             dcn_lr_scale=2.):\n",
    "        super(ConvNormLayer, self).__init__()\n",
    "        assert norm_type in ['bn', 'sync_bn', 'gn', None]\n",
    "        \n",
    "        if not use_dcn:\n",
    "            self.conv = nn.Conv2d(in_channels=ch_in,\n",
    "                                  out_channels=ch_out,\n",
    "                                  kernel_size=filter_size,\n",
    "                                  stride=stride,\n",
    "                                  padding=(filter_size-1)//2,\n",
    "                                  groups=groups,\n",
    "                                  bias=bias_on)\n",
    "        else:\n",
    "            raise NotImplmentedError\n",
    "        \n",
    "        if norm_type in ['bn', 'sync_bn']:\n",
    "            self.norm = nn.BatchNorm2d(ch_out)\n",
    "        else:\n",
    "            self.norm = None\n",
    "    def forward(self, inputs):\n",
    "        out = self.conv(inputs)\n",
    "        if self.norm is not None:\n",
    "            out = self.norm(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class PicoSE(nn.Module):\n",
    "    def __init__(self, feat_channels):\n",
    "        super(PicoSE, self).__init__()\n",
    "        self.fc = nn.Conv2d(feat_channels, feat_channels, 1)\n",
    "        self.conv = ConvNormLayer(feat_channels, feat_channels, 1, 1)\n",
    "    def forward(self, feat, avg_feat):\n",
    "        weight = F.sigmoid(self.fc(avg_feat))\n",
    "        out = self.conv(feat*weight)\n",
    "        return out\n",
    "        \n",
    "    \n",
    "class PicoFeat(nn.Module):\n",
    "    \"\"\"\n",
    "    PicoFeat of PicoDet\n",
    "    Args:\n",
    "        feat_in (int): The channel number of input Tensor.\n",
    "        feat_out (int): The channel number of output Tensor.\n",
    "        num_convs (int): The convolution number of the LiteGFLFeat.\n",
    "        norm_type (str): Normalization type, 'bn'/'sync_bn'/'gn'.\n",
    "        share_cls_reg (bool): Whether to share the cls and reg output.\n",
    "        act (str): The act of per layers.\n",
    "        use_se (bool): Whether to use se module.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 feat_in=256,\n",
    "                 feat_out=96,\n",
    "                 num_fpn_stride=3,\n",
    "                 num_convs=2,\n",
    "                 norm_type='bn',\n",
    "                 share_cls_reg=False,\n",
    "                 act='hard_swish',\n",
    "                 use_se=False):\n",
    "        super(PicoFeat, self).__init__()\n",
    "        self.num_convs = num_convs\n",
    "        self.norm_type = norm_type\n",
    "        self.share_cls_reg = share_cls_reg\n",
    "        self.act = act\n",
    "        self.use_se = use_se\n",
    "        self.cls_convs = nn.ModuleList()\n",
    "        self.reg_convs = nn.ModuleList()\n",
    "        if use_se:\n",
    "            assert share_cls_reg == True, \\\n",
    "                'In the case of using se, share_cls_reg is not supported'\n",
    "            self.se = nn.ModuleList()\n",
    "        for stage_idx in range(num_fpn_stride):\n",
    "            cls_subnet_convs = nn.ModuleList()\n",
    "            reg_subnet_convs = nn.ModuleList()\n",
    "            for i in range(self.num_convs):\n",
    "                in_c = feat_in if i == 0 else feat_out\n",
    "                cls_subnet_convs.append(ConvNormLayer(ch_in=in_c,\n",
    "                                            ch_out=feat_out,\n",
    "                                            filter_size=5,\n",
    "                                            stride=1,\n",
    "                                            groups=feat_out,\n",
    "                                            norm_type=norm_type,\n",
    "                                            bias_on=False,\n",
    "                                            lr_scale=2.))\n",
    "                cls_subnet_convs.append(ConvNormLayer(ch_in=in_c,\n",
    "                                            ch_out=feat_out,\n",
    "                                            filter_size=1,\n",
    "                                            stride=1,\n",
    "                                            norm_type=norm_type,\n",
    "                                            bias_on=False,\n",
    "                                            lr_scale=2.))\n",
    "                if not self.share_cls_reg:\n",
    "                    reg_subnet_convs.append(ConvNormLayer(\n",
    "                                                ch_in=in_c,\n",
    "                                                ch_out=feat_out,\n",
    "                                                filter_size=5,\n",
    "                                                stride=1,\n",
    "                                                groups=feat_out,\n",
    "                                                norm_type=norm_type,\n",
    "                                                bias_on=False,\n",
    "                                                lr_scale=2.))\n",
    "                    reg_subnet_convs.append(ConvNormLayer(\n",
    "                                                ch_in=in_c,\n",
    "                                                ch_out=feat_out,\n",
    "                                                filter_size=1,\n",
    "                                                stride=1,\n",
    "                                                norm_type=norm_type,\n",
    "                                                bias_on=False,\n",
    "                                                lr_scale=2.))\n",
    "                self.cls_convs.append(cls_subnet_convs)\n",
    "                self.reg_convs.append(reg_subnet_convs)\n",
    "                if use_se:\n",
    "                    self.se.append(PicoSE(feat_out))\n",
    "                    \n",
    "        if act == 'hard_swish':\n",
    "            self.act_func = nn.Hardswish()\n",
    "        elif act == 'leaky_relu':\n",
    "            self.act_func = nn.LeakyReLU()\n",
    "        \n",
    "    def forward(self, fpn_feat, stage_idx):\n",
    "        assert stage_idx < len(self.cls_convs)\n",
    "        cls_feat = fpn_feat\n",
    "        reg_feat = fpn_feat\n",
    "        for i in range(len(self.cls_convs[stage_idx])):\n",
    "            cls_feat = self.act_func(self.cls_convs[stage_idx][i](cls_feat))\n",
    "            reg_feat = cls_feat\n",
    "            if not self.share_cls_reg:\n",
    "                reg_feat = self.act_func(self.reg_convs[stage_idx][i](reg_feat))\n",
    "        if self.use_se:\n",
    "            avg_feat = F.adaptive_avg_pool2d(cls_feat, (1,1))\n",
    "            se_feat = self.act_func(self.se[stage_idx](cls_feat, avg_feat))\n",
    "            return cls_feat, se_feat\n",
    "        return cls_feat, reg_feat\n",
    "    \n",
    "class Integral(nn.Module):\n",
    "    \"\"\"A fixed layer for calculating integral result from distribution.\n",
    "    This layer calculates the target location by :math: `sum{P(y_i) * y_i}`,\n",
    "    P(y_i) denotes the softmax vector that represents the discrete distribution\n",
    "    y_i denotes the discrete set, usually {0, 1, 2, ..., reg_max}\n",
    "    Args:\n",
    "        reg_max (int): The maximal value of the discrete set. Default: 16. You\n",
    "            may want to reset it according to your new dataset or related\n",
    "            settings.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, reg_max=16):\n",
    "        super(Intergral, self).__init__()\n",
    "        self.reg_max = reg_max\n",
    "        self.register_buffer\n",
    "    \n",
    "feat = PicoFeat(feat_in=96,\n",
    "                feat_out=96,\n",
    "                num_convs=2,\n",
    "                num_fpn_stride=4,\n",
    "                norm_type='bn',\n",
    "                share_cls_reg=True,\n",
    "                use_se=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "auburn-standing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "eps = 1e-9\n",
    "\n",
    "class PicoHeadV2(nn.Module):\n",
    "    \"\"\"\n",
    "    PicoHeadV2\n",
    "    Args:\n",
    "        conv_feat (object): Instance of 'PicoFeat'\n",
    "        num_classes (int): Number of classes\n",
    "        fpn_stride (list): The stride of each FPN Layer\n",
    "        prior_prob (float): Used to set the bias init for the class prediction layer\n",
    "        loss_class (object): Instance of VariFocalLoss.\n",
    "        loss_dfl (object): Instance of DistributionFocalLoss.\n",
    "        loss_bbox (object): Instance of bbox loss.\n",
    "        assigner (object): Instance of label assigner.\n",
    "        reg_max: Max value of integral set :math: `{0, ..., reg_max}`\n",
    "                n QFL setting. Default: 7.\n",
    "    \"\"\"\n",
    "    __inject__ = [\n",
    "        'conv_feat', 'dgqp_module', 'loss_class', 'loss_dfl', 'loss_bbox',\n",
    "        'static_assigner', 'assigner', 'nms'\n",
    "    ]\n",
    "    __shared__ = ['num_classes', 'eval_size']\n",
    "    \n",
    "    \n",
    "    def __init__(self,\n",
    "                 conv_feat='PicoFeatV2',\n",
    "                 dgqp_module=None,\n",
    "                 num_classes=80,\n",
    "                 fpn_stride=[8, 16, 32],\n",
    "                 prior_prob=0.01,\n",
    "                 use_align_head=True,\n",
    "                 loss_class='VariFocalLoss',\n",
    "                 loss_dfl='DistributionFocalLoss',\n",
    "                 loss_bbox='GIoULoss',\n",
    "                 static_assigner_epoch=60,\n",
    "                 static_assigner='ATSSAssigner',\n",
    "                 assigner='TaskAlignedAssigner',\n",
    "                 reg_max=16,\n",
    "                 feat_in_chan=96,\n",
    "                 nms=None,\n",
    "                 nms_pre=1000,\n",
    "                 cell_offset=0,\n",
    "                 act='hard_swish',\n",
    "                 grid_cell_scale=5.0,\n",
    "                 eval_size=None):\n",
    "            super(PicoHeadV2, self).__init__()\n",
    "            \n",
    "            self.conv_feat = conv_feat\n",
    "            self.num_classes = num_classes\n",
    "            self.fpn_stride = fpn_stride\n",
    "            self.prior_prob = prior_prob\n",
    "            self.loss_vfl = loss_class\n",
    "            self.loss_dfl = loss_dfl\n",
    "            self.loss_bbox = loss_bbox\n",
    "            \n",
    "            self.static_assigner_epoch = static_assigner_epoch\n",
    "            self.static_assigner = static_assigner\n",
    "            self.assigner = assigner\n",
    "            \n",
    "            \n",
    "            self.reg_max = reg_max\n",
    "            self.feat_in_chan = feat_in_chan\n",
    "            self.nms = nms\n",
    "            self.nms_pre = nms_pre\n",
    "            self.cell_offset = cell_offset\n",
    "            self.act = act\n",
    "            self.grid_cell_scale = grid_cell_scale\n",
    "            self.use_align_head = use_align_head\n",
    "            self.cls_out_channels = self.num_classes\n",
    "            self.eval_size = eval_size\n",
    "            \n",
    "            bias_init_value = -math.log((1 - self.prior_prob) / self.prior_prob)\n",
    "            # Clear the super class initialization\n",
    "            self.gfl_head_cls = None\n",
    "            self.gfl_head_reg = None\n",
    "            self.scales_regs = None\n",
    "\n",
    "            #self.distribution_project = Integral(self.reg_max)\n",
    "            \n",
    "            self.head_cls_list = nn.ModuleList()\n",
    "            self.head_reg_list = nn.ModuleList()\n",
    "            self.cls_align = nn.ModuleList()\n",
    "            \n",
    "            for i in range(len(fpn_stride)):\n",
    "                head_cls = nn.Conv2d(\n",
    "                        in_channels=self.feat_in_chan,\n",
    "                        out_channels=self.cls_out_channels,\n",
    "                        kernel_size=1,\n",
    "                        stride=1,\n",
    "                        padding=0)\n",
    "                self.head_cls_list.append(head_cls)\n",
    "                \n",
    "                head_reg =nn.Conv2d(\n",
    "                        in_channels=self.feat_in_chan,\n",
    "                        out_channels=4 * (self.reg_max + 1),\n",
    "                        kernel_size=1,\n",
    "                        stride=1,\n",
    "                        padding=0)\n",
    "                self.head_reg_list.append(head_reg)\n",
    "                \n",
    "                if self.use_align_head:\n",
    "                    self.cls_align.append(DPModule(self.feat_in_chan,\n",
    "                                                   1,\n",
    "                                                   5,\n",
    "                                                   act=self.act,\n",
    "                                                   use_act_in_out=False))\n",
    "                if self.eval_size:\n",
    "                    self.anchor_points, self.stride_tensor = self._generate_anchors()\n",
    "        \n",
    "    def forward_eval(self, fpn_feats, export_post_process=True):\n",
    "        if self.eval_size:\n",
    "            anchor_points, stride_tensor = self.anchor_points, self.stride_tensor\n",
    "        else:\n",
    "            anchor_points, stride_tensor = self._generate_anchors(fpn_feats)\n",
    "        cls_score_list, box_list = [], []\n",
    "        for i, (fpn_feat, stride) in enumerate(zip(fpn_feats, self.fpn_stride)):\n",
    "            b, _, h, w = fpn_feat.shape\n",
    "            # task decomposition\n",
    "            conv_cls_feat, se_feat = self.conv_feat(fpn_feat, i)\n",
    "            cls_logit = self.head_cls_list[i](se_feat)\n",
    "            reg_pred = self.head_reg_list[i](se_feat)\n",
    "\n",
    "            # cls prediction and alignment\n",
    "            if self.use_align_head:\n",
    "                cls_prob = F.sigmoid(self.cls_align[i](conv_cls_feat))\n",
    "                cls_score = (F.sigmoid(cls_logit)*cls_prob+eps).sqrt()\n",
    "            else:\n",
    "                cls_score = F.sigmoid(cls_logit)\n",
    "\n",
    "            if not export_post_process:\n",
    "                cls_score_list.append(cls_score.reshape([1, self.cls_out_channels, -1]).permute([0, 2, 1]))\n",
    "                box_list.append(reg_pred.reshape([1, (self.reg_max+1)*4, -1]).permute([0, 2, 1]))\n",
    "\n",
    "            else:\n",
    "                l = h*w\n",
    "                cls_score_out = cls_score.reshape([b, self.cls_out_channels, l])\n",
    "                print(reg_pred.size())\n",
    "                bbox_pred = reg_pred.permute([0, 2, 3, 1])\n",
    "                bbox_pred = self.distribution_project(bbox_pred)\n",
    "                bbox_pred = bbox_pred.reshape([b, l, 4])\n",
    "                cls_score_list.append(cls_score_out)\n",
    "                box_list.append(bbox_pred)\n",
    "\n",
    "        if export_post_process:\n",
    "            cls_score_list = torch.concat(cls_score_list, dim=-1)\n",
    "            box_list = torch.concat(box_list, dim=1)\n",
    "            box_list = batch_distance2bbox(anchor_points, box_list)\n",
    "            box_list *= stride_tensor\n",
    "        return cls_score_list, box_list\n",
    "              \n",
    "            \n",
    "        \n",
    "    def forward(self, fpn_feats, export_post_process=True):\n",
    "        # only made for evaluation for now\n",
    "        return self.forward_eval(fpn_feats, export_post_process)\n",
    "        \n",
    "    def _generate_anchors(self, feats=None):\n",
    "        # just use in eval time\n",
    "        anchor_points = []\n",
    "        stride_tensor = []\n",
    "        for i, stride in enumerate(self.fpn_stride):\n",
    "            if feats is not None:\n",
    "                _, _, h, w = feats[i].shape\n",
    "            else:\n",
    "                h = math.ceil(self.eval_size[0] / stride)\n",
    "                w = math.ceil(self.eval_size[1] / stride)\n",
    "            shift_x = torch.arange(end=w) + self.cell_offset\n",
    "            shift_y = torch.arange(end=h) + self.cell_offset\n",
    "            shift_y, shift_x = torch.meshgrid(shift_y, shift_x)\n",
    "            anchor_point =torch.stack(\n",
    "                    [shift_x, shift_y], axis=-1).float()\n",
    "            anchor_points.append(anchor_point.reshape([-1, 2]))\n",
    "            stride_tensor.append(\n",
    "                torch.full(\n",
    "                    [h * w, 1], stride))\n",
    "        anchor_points = torch.concat(anchor_points)\n",
    "        stride_tensor = torch.concat(stride_tensor)\n",
    "        return anchor_points, stride_tensor\n",
    "    def post_process(self, head_outs, scale_factor, export_nms=True):\n",
    "        pass\n",
    "\n",
    "feat = PicoFeat(feat_in=96,\n",
    "                feat_out=96,\n",
    "                num_convs=2,\n",
    "                num_fpn_stride=4,\n",
    "                norm_type='bn',\n",
    "                share_cls_reg=True,\n",
    "                use_se=True)\n",
    "\n",
    "# test constructor works\n",
    "head = PicoHeadV2(conv_feat=feat, feat_in_chan=96)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signal-money",
   "metadata": {},
   "source": [
    "# Test End-to-End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "future-shell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "torch.Size([1, 1600, 80])\n",
      "3\n",
      "torch.Size([1, 1600, 68])\n"
     ]
    }
   ],
   "source": [
    "# model config\n",
    "scale = 0.75\n",
    "feature_maps = [3, 4, 5]\n",
    "out_channels = 96\n",
    "\n",
    "backbone = LCNet(scale, feature_maps)\n",
    "# get backbone output shape\n",
    "outputs = backbone({'image': torch.randn(1, 3, 320, 320)})\n",
    "in_channels = [c.size()[1] for c in outputs]\n",
    "\n",
    "neck = LCPAN(in_channels, out_channels)\n",
    "outputs = neck(outputs)\n",
    "\n",
    "feat = PicoFeat(feat_in=96,\n",
    "                feat_out=96,\n",
    "                num_convs=2,\n",
    "                num_fpn_stride=4,\n",
    "                norm_type='bn',\n",
    "                share_cls_reg=True,\n",
    "                use_se=True)\n",
    "\n",
    "head = PicoHeadV2(conv_feat=feat, feat_in_chan=96)\n",
    "\n",
    "outputs = head(outputs, export_post_process=False)\n",
    "cls_score, bbox_list = outputs\n",
    "print(len(cls_score))\n",
    "print(cls_score[0].size())\n",
    "print(len(bbox_list))\n",
    "print(bbox_list[0].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nonprofit-tumor",
   "metadata": {},
   "source": [
    "# Picodet full network: https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.4/ppdet/modeling/architectures/picodet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "radio-blair",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PicoDetV2:\n",
    "    def __init__(self, backbone, neck, head):\n",
    "        self.backbone = backbone\n",
    "        self.neck = neck\n",
    "        self.head = head\n",
    "        self.export_post_process = True\n",
    "        self.export_nms = True\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        body_feats = self.backbone(inputs)\n",
    "        fpn_feats = self.neck(body_feats)\n",
    "        head_outs = self.head(fpn_feats, self.export_post_process)\n",
    "        \n",
    "        # post process\n",
    "        scale_factor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchenv",
   "language": "python",
   "name": "pytorchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
